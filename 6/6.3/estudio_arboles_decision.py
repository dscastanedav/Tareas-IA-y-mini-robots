"""
ESTUDIO DETALLADO DEL ALGORITMO √ÅRBOLES DE DECISI√ìN
===================================================

Este script contiene un estudio completo del algoritmo de √°rboles de decisi√≥n:
- Explicaci√≥n te√≥rica detallada
- Implementaci√≥n pr√°ctica
- Ejemplo de aplicaci√≥n web comentado
- Documentaci√≥n mejorada para entender cada parte
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import LabelEncoder
import warnings
warnings.filterwarnings('ignore')

class EstudioArbolesDecision:
    """
    Clase para estudiar el algoritmo de √°rboles de decisi√≥n de forma detallada
    
    Los √°rboles de decisi√≥n son algoritmos de ML que:
    1. Toman decisiones siguiendo un flujo de preguntas
    2. Dividen los datos en base a caracter√≠sticas
    3. Crean reglas claras y comprensibles
    4. Pueden usarse para clasificaci√≥n y regresi√≥n
    """
    
    def __init__(self):
        """
        Inicializa el estudio de √°rboles de decisi√≥n
        """
        self.modelo = None
        self.label_encoder = LabelEncoder()
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.feature_names = None
        
    def explicar_teoria_arboles(self):
        """
        Explica la teor√≠a detr√°s de los √°rboles de decisi√≥n
        """
        print("="*80)
        print("TEOR√çA DE LOS √ÅRBOLES DE DECISI√ìN")
        print("="*80)
        
        print("\nüìö ¬øQU√â SON LOS √ÅRBOLES DE DECISI√ìN?")
        print("   Los √°rboles de decisi√≥n son algoritmos que:")
        print("   ‚Ä¢ Toman decisiones siguiendo un flujo de preguntas")
        print("   ‚Ä¢ Dividen los datos en base a caracter√≠sticas")
        print("   ‚Ä¢ Crean reglas claras y f√°ciles de entender")
        print("   ‚Ä¢ Funcionan como un diagrama de flujo")
        
        print("\nüå≥ ESTRUCTURA DE UN √ÅRBOL:")
        print("   ‚Ä¢ NODO RA√çZ: Primera pregunta/decisi√≥n")
        print("   ‚Ä¢ NODOS INTERNOS: Preguntas intermedias")
        print("   ‚Ä¢ HOJAS: Decisiones finales (clasificaciones)")
        print("   ‚Ä¢ RAMAS: Caminos entre nodos")
        
        print("\nüîç CONCEPTOS CLAVE:")
        print("   1. ENTROP√çA: Medida de desorden/impureza")
        print("   2. GANANCIA DE INFORMACI√ìN: Reducci√≥n de entrop√≠a")
        print("   3. GINI: Medida alternativa de impureza")
        print("   4. PODA: Eliminar ramas para evitar overfitting")
        
        print("\n‚öôÔ∏è CRITERIOS DE DIVISI√ìN:")
        print("   ‚Ä¢ GINI: Medida de impureza (m√°s com√∫n)")
        print("   ‚Ä¢ ENTROPY: Basado en teor√≠a de informaci√≥n")
        print("   ‚Ä¢ Busca la divisi√≥n que mejor separe las clases")
        
        print("\nüéØ VENTAJAS:")
        print("   ‚úÖ F√°cil de entender e interpretar")
        print("   ‚úÖ No requiere normalizaci√≥n de datos")
        print("   ‚úÖ Maneja datos categ√≥ricos y num√©ricos")
        print("   ‚úÖ Identifica caracter√≠sticas importantes")
        print("   ‚úÖ R√°pido para predicciones")
        
        print("\n‚ö†Ô∏è DESVENTAJAS:")
        print("   ‚ùå Propenso a overfitting")
        print("   ‚ùå Inestable (peque√±os cambios ‚Üí √°rboles diferentes)")
        print("   ‚ùå Sesgado hacia caracter√≠sticas con m√°s valores")
        print("   ‚ùå Dificultad con relaciones lineales")
        
    def generar_datos_ejemplo(self):
        """
        Genera un dataset simple para demostrar √°rboles de decisi√≥n
        """
        print("\n" + "="*80)
        print("GENERACI√ìN DE DATOS DE EJEMPLO")
        print("="*80)
        
        # Crear dataset sint√©tico para clasificaci√≥n
        np.random.seed(42)
        
        # Caracter√≠sticas: Edad, Salario, Experiencia
        edad = np.random.randint(20, 65, 200)
        salario = np.random.randint(30000, 120000, 200)
        experiencia = np.random.randint(0, 30, 200)
        
        # Reglas para crear las clases (simulando aprobaci√≥n de pr√©stamo)
        aprobado = np.zeros(200)
        for i in range(200):
            # Reglas simples para aprobar pr√©stamo
            if edad[i] >= 25 and salario[i] >= 50000 and experiencia[i] >= 2:
                aprobado[i] = 1
            elif salario[i] >= 80000:
                aprobado[i] = 1
            elif edad[i] >= 40 and experiencia[i] >= 10:
                aprobado[i] = 1
            # Agregar algo de ruido
            if np.random.random() < 0.1:
                aprobado[i] = 1 - aprobado[i]
        
        # Crear dataset
        X = np.column_stack((edad, salario, experiencia))
        y = aprobado.astype(int)
        
        # Nombres de caracter√≠sticas
        self.feature_names = ['Edad', 'Salario', 'Experiencia']
        
        print(f"   ‚úÖ Dataset generado: {len(X)} muestras")
        print(f"   ‚Ä¢ Caracter√≠sticas: {X.shape[1]} (Edad, Salario, Experiencia)")
        print(f"   ‚Ä¢ Clase 0 (Rechazado): {np.sum(y == 0)} muestras")
        print(f"   ‚Ä¢ Clase 1 (Aprobado): {np.sum(y == 1)} muestras")
        print(f"   ‚Ä¢ Problema: Aprobaci√≥n de pr√©stamos")
        
        return X, y
    
    def entrenar_arbol(self, X, y):
        """
        Entrena diferentes √°rboles de decisi√≥n con distintos par√°metros
        """
        print("\n" + "="*80)
        print("ENTRENAMIENTO DE √ÅRBOLES DE DECISI√ìN")
        print("="*80)
        
        # Dividir datos
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=0.3, random_state=42
        )
        
        print(f"   üìä Datos de entrenamiento: {len(self.X_train)} muestras")
        print(f"   üìä Datos de prueba: {len(self.X_test)} muestras")
        
        # Entrenar diferentes √°rboles
        modelos = {
            '√Årbol Simple': DecisionTreeClassifier(random_state=42),
            '√Årbol Limitado': DecisionTreeClassifier(
                max_depth=5, 
                min_samples_split=10, 
                random_state=42
            ),
            '√Årbol con Gini': DecisionTreeClassifier(
                criterion='gini', 
                max_depth=4, 
                random_state=42
            ),
            '√Årbol con Entrop√≠a': DecisionTreeClassifier(
                criterion='entropy', 
                max_depth=4, 
                random_state=42
            )
        }
        
        resultados = {}
        
        for nombre, modelo in modelos.items():
            print(f"\n   üå≥ Entrenando {nombre}...")
            
            # Entrenar modelo
            modelo.fit(self.X_train, self.y_train)
            
            # Hacer predicciones
            y_pred = modelo.predict(self.X_test)
            
            # Calcular m√©tricas
            accuracy = accuracy_score(self.y_test, y_pred)
            
            # Informaci√≥n del √°rbol
            n_nodes = modelo.tree_.node_count
            depth = modelo.tree_.max_depth
            
            resultados[nombre] = {
                'modelo': modelo,
                'accuracy': accuracy,
                'predicciones': y_pred,
                'nodos': n_nodes,
                'profundidad': depth
            }
            
            print(f"   ‚úÖ {nombre}: Precisi√≥n = {accuracy:.4f}")
            print(f"      ‚Ä¢ Nodos: {n_nodes}, Profundidad: {depth}")
        
        # Encontrar mejor modelo
        mejor_modelo = max(resultados.items(), key=lambda x: x[1]['accuracy'])
        self.modelo = mejor_modelo[1]['modelo']
        
        print(f"\n   üèÜ MEJOR MODELO: {mejor_modelo[0]} (Precisi√≥n: {mejor_modelo[1]['accuracy']:.4f})")
        
        return resultados
    
    def analizar_importancia_caracteristicas(self):
        """
        Analiza la importancia de las caracter√≠sticas
        """
        print("\n" + "="*80)
        print("AN√ÅLISIS DE IMPORTANCIA DE CARACTER√çSTICAS")
        print("="*80)
        
        importancias = self.modelo.feature_importances_
        
        print("   üìä Importancia de cada caracter√≠stica:")
        for i, (nombre, importancia) in enumerate(zip(self.feature_names, importancias)):
            print(f"   ‚Ä¢ {nombre}: {importancia:.4f} ({importancia*100:.1f}%)")
        
        # Encontrar la caracter√≠stica m√°s importante
        mas_importante = np.argmax(importancias)
        print(f"\n   üèÜ CARACTER√çSTICA M√ÅS IMPORTANTE: {self.feature_names[mas_importante]}")
        
        return importancias
    
    def mostrar_reglas_decision(self):
        """
        Muestra las reglas de decisi√≥n del √°rbol
        """
        print("\n" + "="*80)
        print("REGLAS DE DECISI√ìN DEL √ÅRBOL")
        print("="*80)
        
        # Extraer reglas del √°rbol
        tree = self.modelo.tree_
        feature_names = self.feature_names
        
        def get_rules(node, depth=0):
            """Funci√≥n recursiva para extraer reglas"""
            indent = "  " * depth
            
            if tree.feature[node] != -2:  # No es una hoja
                feature = feature_names[tree.feature[node]]
                threshold = tree.threshold[node]
                
                print(f"{indent}SI {feature} <= {threshold:.2f}:")
                get_rules(tree.children_left[node], depth + 1)
                
                print(f"{indent}SI {feature} > {threshold:.2f}:")
                get_rules(tree.children_right[node], depth + 1)
            else:  # Es una hoja
                clase = np.argmax(tree.value[node])
                confianza = np.max(tree.value[node]) / np.sum(tree.value[node])
                resultado = "APROBADO" if clase == 1 else "RECHAZADO"
                print(f"{indent}‚Üí {resultado} (confianza: {confianza:.2f})")
        
        print("   üå≥ Reglas extra√≠das del √°rbol:")
        get_rules(0)
        
    def analizar_resultados(self, resultados):
        """
        Analiza los resultados de los diferentes modelos
        """
        print("\n" + "="*80)
        print("AN√ÅLISIS COMPARATIVO DE MODELOS")
        print("="*80)
        
        for nombre, datos in resultados.items():
            print(f"\nüìà MODELO: {nombre}")
            print(f"   ‚Ä¢ Precisi√≥n: {datos['accuracy']:.4f}")
            print(f"   ‚Ä¢ Nodos: {datos['nodos']}")
            print(f"   ‚Ä¢ Profundidad: {datos['profundidad']}")
            
            # Matriz de confusi√≥n
            cm = confusion_matrix(self.y_test, datos['predicciones'])
            print(f"   ‚Ä¢ Matriz de confusi√≥n:")
            print(f"     Rechazados correctos: {cm[0,0]}")
            print(f"     Falsos aprobados: {cm[0,1]}")
            print(f"     Falsos rechazados: {cm[1,0]}")
            print(f"     Aprobados correctos: {cm[1,1]}")
            
            # Interpretaci√≥n del negocio
            if cm[0,1] > 0:
                print(f"   ‚ö†Ô∏è Riesgo: {cm[0,1]} pr√©stamos riesgosos aprobados")
            if cm[1,0] > 0:
                print(f"   üí∞ P√©rdida: {cm[1,0]} buenos clientes rechazados")
    
    def visualizar_arbol(self, X, y, resultados):
        """
        Crea visualizaciones del √°rbol de decisi√≥n
        """
        print("\n" + "="*80)
        print("GENERANDO VISUALIZACIONES")
        print("="*80)
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Estudio de √Årboles de Decisi√≥n', fontsize=16)
        
        # Visualizar el √°rbol
        axes[0, 0].set_title('Estructura del Mejor √Årbol')
        plot_tree(self.modelo, 
                 feature_names=self.feature_names,
                 class_names=['Rechazado', 'Aprobado'],
                 filled=True,
                 rounded=True,
                 ax=axes[0, 0])
        
        # Importancia de caracter√≠sticas
        importancias = self.modelo.feature_importances_
        axes[0, 1].bar(self.feature_names, importancias, color=['skyblue', 'lightgreen', 'lightcoral'])
        axes[0, 1].set_title('Importancia de Caracter√≠sticas')
        axes[0, 1].set_ylabel('Importancia')
        
        # Comparaci√≥n de precisiones
        nombres = list(resultados.keys())
        precisiones = [resultados[nombre]['accuracy'] for nombre in nombres]
        
        axes[1, 0].bar(range(len(nombres)), precisiones, color='lightblue')
        axes[1, 0].set_title('Comparaci√≥n de Precisiones')
        axes[1, 0].set_ylabel('Precisi√≥n')
        axes[1, 0].set_xticks(range(len(nombres)))
        axes[1, 0].set_xticklabels(nombres, rotation=45)
        
        # Informaci√≥n del modelo
        axes[1, 1].text(0.1, 0.8, f'RESUMEN DEL ESTUDIO', fontsize=14, weight='bold', transform=axes[1, 1].transAxes)
        axes[1, 1].text(0.1, 0.7, f'Mejor Modelo: {max(resultados.items(), key=lambda x: x[1]["accuracy"])[0]}', 
                        fontsize=12, transform=axes[1, 1].transAxes)
        axes[1, 1].text(0.1, 0.6, f'Precisi√≥n: {max(resultados.values(), key=lambda x: x["accuracy"])["accuracy"]:.4f}', 
                        fontsize=12, transform=axes[1, 1].transAxes)
        axes[1, 1].text(0.1, 0.5, f'Muestras: {len(X)}', fontsize=12, transform=axes[1, 1].transAxes)
        axes[1, 1].text(0.1, 0.4, f'Caracter√≠sticas: {X.shape[1]}', fontsize=12, transform=axes[1, 1].transAxes)
        
        axes[1, 1].text(0.1, 0.2, 'Ventajas de √Årboles:', fontsize=12, weight='bold', transform=axes[1, 1].transAxes)
        axes[1, 1].text(0.1, 0.15, '‚Ä¢ F√°cil interpretaci√≥n', fontsize=10, transform=axes[1, 1].transAxes)
        axes[1, 1].text(0.1, 0.1, '‚Ä¢ Reglas claras', fontsize=10, transform=axes[1, 1].transAxes)
        axes[1, 1].text(0.1, 0.05, '‚Ä¢ No requiere normalizaci√≥n', fontsize=10, transform=axes[1, 1].transAxes)
        
        axes[1, 1].set_xlim(0, 1)
        axes[1, 1].set_ylim(0, 1)
        axes[1, 1].axis('off')
        
        plt.tight_layout()
        plt.savefig('estudio_arboles.png', 
                   dpi=300, bbox_inches='tight')
        plt.show()
        
        print("   ‚úÖ Visualizaciones guardadas como 'estudio_arboles.png'")

def main():
    """
    Funci√≥n principal que ejecuta el estudio completo de √°rboles de decisi√≥n
    """
    print("="*80)
    print("ESTUDIO COMPLETO DE √ÅRBOLES DE DECISI√ìN")
    print("="*80)
    
    # Crear instancia del estudio
    estudio = EstudioArbolesDecision()
    
    # 1. Explicar la teor√≠a
    estudio.explicar_teoria_arboles()
    
    # 2. Generar datos
    X, y = estudio.generar_datos_ejemplo()
    
    # 3. Entrenar modelos
    resultados = estudio.entrenar_arbol(X, y)
    
    # 4. Analizar importancia
    estudio.analizar_importancia_caracteristicas()
    
    # 5. Mostrar reglas
    estudio.mostrar_reglas_decision()
    
    # 6. Analizar resultados
    estudio.analizar_resultados(resultados)
    
    # 7. Visualizar
    estudio.visualizar_arbol(X, y, resultados)
    
    print("\n" + "="*80)
    print("ESTUDIO DE √ÅRBOLES DE DECISI√ìN COMPLETADO")
    print("="*80)

if __name__ == "__main__":
    main()

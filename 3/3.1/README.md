# ğŸ¯ MaximizaciÃ³n de FunciÃ³n

## 3.1 OptimizaciÃ³n de f(x) = xÂ·sen(10Ï€x) + 1

### ğŸ“‹ DescripciÃ³n del Problema

Este proyecto implementa diferentes mÃ©todos para encontrar el **mÃ¡ximo global** de la funciÃ³n:

```
f(x) = x Â· sen(10Ï€x) + 1
```

En el intervalo **x âˆˆ [0,1]**.

### ğŸ¯ Objetivo

Encontrar el valor de **x*** que maximiza la funciÃ³n, es decir:

```
x* = arg max f(x)    para x âˆˆ [0,1]
x*
```

---

## ğŸ”¬ AnÃ¡lisis de la FunciÃ³n

### CaracterÃ­sticas MatemÃ¡ticas

- **Dominio**: [0, 1]
- **Rango**: Aproximadamente [0, 2]
- **Tipo**: FunciÃ³n multimodal (mÃºltiples mÃ¡ximos locales)
- **Derivada**: f'(x) = sen(10Ï€x) + 10Ï€xÂ·cos(10Ï€x)

### Comportamiento

1. **MÃºltiples mÃ¡ximos locales**: Debido al tÃ©rmino sen(10Ï€x)
2. **Crecimiento general**: El factor x hace que los valores aumenten hacia x=1
3. **Oscilaciones rÃ¡pidas**: 10Ï€ â‰ˆ 31.4 oscilaciones en [0,1]

---

## ğŸ› ï¸ MÃ©todos Implementados

### 1. ğŸ”¨ Fuerza Bruta
- **DescripciÃ³n**: EvalÃºa la funciÃ³n en una grilla muy fina
- **Ventajas**: Garantiza encontrar el Ã³ptimo global
- **Desventajas**: Computacionalmente costoso

### 2. ğŸ”¬ SciPy Brute Force
- **DescripciÃ³n**: Usa `scipy.optimize.brute`
- **Ventajas**: ImplementaciÃ³n optimizada
- **Desventajas**: Limitado por el tamaÃ±o de grilla

### 3. ğŸ¥‡ SecciÃ³n Dorada
- **DescripciÃ³n**: MÃ©todo clÃ¡sico de optimizaciÃ³n
- **Ventajas**: RÃ¡pido y eficiente
- **Desventajas**: Solo encuentra mÃ¡ximo local

### 4. ğŸ§¬ EvoluciÃ³n Diferencial
- **DescripciÃ³n**: Algoritmo evolutivo global
- **Ventajas**: Excelente para optimizaciÃ³n global
- **Desventajas**: EstocÃ¡stico, puede variar

### 5. ğŸ² BÃºsqueda Aleatoria
- **DescripciÃ³n**: Monte Carlo con puntos aleatorios
- **Ventajas**: Simple y robusto
- **Desventajas**: Requiere muchas evaluaciones

### 6. ğŸ“ AnÃ¡lisis de Derivada
- **DescripciÃ³n**: Encuentra puntos crÃ­ticos analÃ­ticamente
- **Ventajas**: MatemÃ¡ticamente riguroso
- **Desventajas**: Complejo para funciones oscilatorias

---

## ğŸš€ CÃ³mo Ejecutar

### Requisitos

```bash
pip install numpy matplotlib scipy
```

### EjecuciÃ³n

```bash
cd "c:\Users\santi\OneDrive\Documentos\Tareas IA y mini robots\3\3.1"
python maximizacion_funcion.py
```

### Salida Esperada

1. **AnÃ¡lisis de la funciÃ³n**: GrÃ¡fico y caracterÃ­sticas
2. **ComparaciÃ³n de mÃ©todos**: Resultados de cada algoritmo
3. **Mejor resultado**: MÃ©todo ganador y visualizaciÃ³n
4. **Archivos generados**:
   - `analisis_funcion.png`
   - `resultado_final.png`

---

## ğŸ“Š Resultados Esperados

### MÃ¡ximo Global Aproximado
- **x*** â‰ˆ 0.95 - 0.98
- **f(x*)** â‰ˆ 1.85 - 1.95

### MÃ©todos MÃ¡s Confiables
1. **EvoluciÃ³n Diferencial**: Mejor balance precisiÃ³n/eficiencia
2. **Fuerza Bruta**: MÃ¡s preciso pero mÃ¡s lento
3. **AnÃ¡lisis Derivada**: MatemÃ¡ticamente riguroso

---

## ğŸ” InterpretaciÃ³n MatemÃ¡tica

### Â¿Por quÃ© es DifÃ­cil este Problema?

1. **FunciÃ³n multimodal**: Muchos mÃ¡ximos locales
2. **Oscilaciones rÃ¡pidas**: 10Ï€ ciclos en [0,1]
3. **Tendencia creciente**: El factor x sesga hacia x=1

### Estrategias de SoluciÃ³n

- **MÃ©todos globales**: Para evitar mÃ¡ximos locales
- **Alta resoluciÃ³n**: Para capturar oscilaciones finas
- **CombinaciÃ³n de mÃ©todos**: ValidaciÃ³n cruzada

---

## ğŸ“ˆ AnÃ¡lisis de Sensibilidad

### Efecto de ParÃ¡metros

```python
# Frecuencia mÃ¡s baja: f(x) = xÂ·sin(2Ï€x) + 1
# Resultado: Menos mÃ¡ximos locales, mÃ¡s fÃ¡cil

# Frecuencia mÃ¡s alta: f(x) = xÂ·sin(20Ï€x) + 1  
# Resultado: MÃ¡s mÃ¡ximos locales, mÃ¡s difÃ­cil
```

### ValidaciÃ³n de Resultados

1. **Verificar dominio**: x* âˆˆ [0,1]
2. **Comparar mÃ©todos**: Consenso entre algoritmos
3. **AnÃ¡lisis visual**: Confirmar en grÃ¡fico

---

## ğŸ›ï¸ Variaciones del Problema

### Funciones Similares

```python
# VariaciÃ³n 1: Cambiar amplitud
f1(x) = 2*x * sin(10*Ï€*x) + 1

# VariaciÃ³n 2: Cambiar frecuencia  
f2(x) = x * sin(5*Ï€*x) + 1

# VariaciÃ³n 3: FunciÃ³n diferente
f3(x) = xÂ² * sin(10*Ï€*x) + 1
```

### Extensiones Posibles

1. **OptimizaciÃ³n multi-objetivo**
2. **Restricciones adicionales**
3. **FunciÃ³n en mÃºltiples variables**
4. **OptimizaciÃ³n dinÃ¡mica**

---

## ğŸ”§ Troubleshooting

### Problemas Comunes

**Error de importaciÃ³n**:
```bash
pip install --upgrade scipy matplotlib numpy
```

**GrÃ¡ficos no aparecen**:
```python
plt.show(block=True)
```

**Resultados inconsistentes**:
- Verificar semillas aleatorias
- Aumentar nÃºmero de evaluaciones
- Usar mÃºltiples ejecuciones

---

## ğŸ“š Referencias TeÃ³ricas

### OptimizaciÃ³n Global
- **Algoritmos evolutivos**: Holland, Goldberg
- **Simulated Annealing**: Kirkpatrick et al.
- **Particle Swarm**: Kennedy & Eberhart

### AnÃ¡lisis de Funciones
- **AnÃ¡lisis de Fourier**: Para funciones oscilatorias
- **TeorÃ­a de aproximaciÃ³n**: MÃ©todos numÃ©ricos
- **CÃ¡lculo variacional**: OptimizaciÃ³n continua

---

## ğŸ¯ Objetivos de Aprendizaje

DespuÃ©s de este ejercicio deberÃ­as entender:

1. **OptimizaciÃ³n global vs local**
2. **MÃ©todos determinÃ­sticos vs estocÃ¡sticos**
3. **AnÃ¡lisis de funciones multimodales**
4. **ComparaciÃ³n de algoritmos**
5. **ValidaciÃ³n de resultados numÃ©ricos**

---

**Nota**: Este es un problema clÃ¡sico en optimizaciÃ³n global que ilustra las dificultades de encontrar Ã³ptimos en funciones oscilatorias.
